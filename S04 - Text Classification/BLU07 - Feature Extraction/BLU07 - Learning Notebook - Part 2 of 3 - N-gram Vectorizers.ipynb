{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "from numpy import inf\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at some Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning all about tokenization and regexes, let's start doing some cool stuff and apply it in a true dataset!\n",
    "\n",
    "In Part II of this BLU, we're going to look into how to transform text into something that is meaningful to a machine. As you may have noticed, text is a bit different from other datasets you might have seen - it's just a bunch of words stringed together! Where are the features in a tightly organized table of examples? Unlike other data you might have worked with in previous BLU's, text is unstructured and thus needs some additional work on our end to make it structured and ready to be handled by a machine learning algorithm.\n",
    "\n",
    "<img src=\"./media/xkcd_language_nerd.png\" width=\"300\">\n",
    "\n",
    "Language can be messy. One thing is clear - we need features. To get features from a string of text, or a **document**, is to **vectorize** it. Normally, this means that our feature space is the **vocabulary** of the examples present in our dataset, that is, the set of unique words we can find in all of the training examples.\n",
    "\n",
    "But enough talk - let's get our hands dirty!\n",
    "\n",
    "In this BLU, we're going to work with some movie reviews from IMDB. Let's load the dataset into pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are two columns in this dataset - one for the labels and another for the text of the movie review. Each example is labeled as a positive or negative review. Our goal is to retrieve meaningful features from the text so a machine can predict if a given unlabeled review is positive or negative.\n",
    "\n",
    "Let's see a positive and a negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example = df.text[4835]\n",
    "print(df.sentiment[4835])\n",
    "print(pos_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So here's a review about *The Lion King 1 and a 1/2* (a.k.a. *The Lion King 3* in some countries). It seems the reviewer liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_example = df.text[4]\n",
    "print(df.sentiment[4])\n",
    "print(neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. I guess that's a pass for this one, right?\n",
    "\n",
    "Let's get the first 200 documents of this dataset to run experiments faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in Part I, we can tokenize and stem our text to be able to extract better features. Let's initialize our favorite tokenizer and stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a regex to clean our sentences. We can see from the examples above that our corpus has some html substrings `<br />` that are only polluting the sentences. We can remove them with `re.sub()` by substituting every substring that matches the regex `<[^>]*>` with an empty string.\n",
    "\n",
    "We will define a `clean()` method that removes these unnecessary html tags, tokenizes and stems our corpus' sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "    # stem\n",
    "    stems = list(map(stemmer.stem, words))\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one of the above examples again, after we cleaned the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we may not understand it as well now, but we actually just made the text much easier for a machine to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, our feature space in text will be the vocabulary of our data. In our example, this is the set of unique words and symbols present in our documents.\n",
    "\n",
    "To create our vocabulary, we will use a `Counter()`. `Counter()` is a dictionary that counts the number of ocurrences of different tokens in a list and can be updated with each sentence of our corpus.\n",
    "\n",
    "After getting all counts for each unique token, we sort our dictionary by counts using `Counter()`'s built-in method `.most_common()`, and store everything in an `OrderedDict()`. This makes sure our vectorized representations of the documents will be ordered according to the most common words in the whole corpus (this is not required, but makes data visualization much nicer!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "# turn into a list of tuples and get the first 20 items\n",
    "list(build_vocabulary().items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text through a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary, we can vectorize our documents. The value of our features will be the most simple vectorization of a document there is - the word counts.\n",
    "\n",
    "By doing this, each column value is the number of times that word of the vocabulary appeared in the document. This is what is called a **Bag of Words** (BoW) representation.\n",
    "\n",
    "Note that this type of vectorization of the document loses all the syntactic information of it. That is, you could shuffle the words in document and get the same vector (that's why it's called a bag of words). Of course, since we are trying to understand if a movie review is positive or negative, one could argue that what really matters as features is what kind of words appear in the documents and not their order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this better if we use a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df():\n",
    "    return pd.DataFrame(vectorize(), columns=build_vocabulary())\n",
    "\n",
    "build_df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for the most meaningful features in our vocabulary to tell us in what category our document falls into. Text is filled with words that are unimportant to the meaning of a particular sentence like \"the\" or \"and\". This is in contrast with words like \"love\" or \"hate\" that have a very clear semantic meaning. The former example of words are called **stopwords** - words that _usually_ don't introduce any meaning to a piece of text and are often just in the document for syntactic reasons.\n",
    "\n",
    "It's important to emphasize that we used \"usually\" in our previous statement. You should be aware that sometimes stopwords can be useful features, especially when we use more than just unigrams as features (ex.: bigrams, trigrams, ...) and word order and word combination starts to be relevant.\n",
    " \n",
    "You can easily find lists of stopwords for several languages in the internet. You can find one for english in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [line.strip(\"\\n\") for line in open(\"./data/english_stopwords.txt\", \"r\")]\n",
    "\n",
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our build_vocabulary() and vectorize() functions and remove these words from the text. This way we will reduce our vocabulary - and thus our feature space - making our representations more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in stopwords]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize():\n",
    "    vocabulary = build_vocabulary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary if word not in stopwords])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "BoW = build_df()\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we could do is to normalize our counts. As you can see, different documents have different number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoW.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can introduce bias in our features, so we should normalize each document by its number of words. This way, instead of having word counts as features of our model, we will have **term frequencies**. This way, the features in any document of the dataset sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = BoW.div(BoW.sum(axis=1), axis=0)\n",
    "tf.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kicking it up a notch with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear by now that not all words have the same importance to find out in what category a document falls into.\n",
    "\n",
    "In our dataset, if we want to classify a review as positive, for instance, the word \"*good*\" is much more informative than \"*house*\", and we should give it more weight as a feature.\n",
    "\n",
    "In general, words that are very common in the corpus are less informative than rare words.\n",
    "\n",
    "That is the rational behind **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "\n",
    "$$ tfidf _{t, d} =(log_2{(1 + tf_{t,d})})*(log_2{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "where $t$ and $d$ are the term and document for which we are computing a feature, $tf_{t,d}$ is the term frequency of term $t$ in document $d$, $N$ is the total number of documents we have, while $df_{t}$ is the number of documents that contain $t$.\n",
    "\n",
    "We are using the word frequencies we were using before, but now we are weighting each by the inverse of the number of times they occur in all the documents. The more a word appears in a document and the less it appears in other documents, the higher is the TF-IDF of that word in that document.\n",
    "\n",
    "In short, we measure **the term frequency, weighted by its rarity in the entire corpus**.\n",
    "\n",
    "**Note**: TF-IDF can vary in formulation - the idea is always the same but computation might change slightly. In our case, we are choosing to log-normalize our frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(column):\n",
    "    return np.log2(1 + len(column) / sum(column > 0))\n",
    "\n",
    "tf_idf = (np.log2(1 + tf)).multiply(tf.apply(idf))\n",
    "\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing we can do with our feature representations is to check similarities between words. To do this, instead of seeing the vocabulary as the features of a given document, we see each document as a feature of a given term in the vocabulary. By doing this, we get a vectorized representation of a word!\n",
    "\n",
    "A very popular way of computing similarities between vectors is to compute the cosine similarity.\n",
    "\n",
    "Let's check the similarity between the word _movi_ (which is the stem of *movie*) and the word *film* in our Bag of Words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(BoW['movi'].values.reshape(1,-1), BoW['film'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute it for _movi_ and _shoe_. We should get a lower similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(BoW['movi'].__array__().reshape(1,-1), BoW['shoe'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the same similarity scores but in the tf-idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(tf_idf['movi'].__array__().reshape(1,-1), tf_idf['film'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(tf_idf['movi'].__array__().reshape(1,-1), tf_idf['shoe'].__array__().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The gap between the similarities of these pair of words increased with our tf-idf representation. This means that our tf-idf model is computing better and more meaningful features than our BoW model. This will surely help when we feed these feature matrices to a classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

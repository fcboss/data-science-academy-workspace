{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bdd5aa2da3067e63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU08 - Classification With Logistic Regression: Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2a09fb80383b343",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27488522ce0b142e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following: \n",
    "\n",
    "    - What classification is for\n",
    "    - Logistic regression\n",
    "    - Cost function\n",
    "    - Binary classification\n",
    "    \n",
    "You thought that you would get away without implementing your own little Logistic Regression? Hah!\n",
    "\n",
    "\n",
    "# Exercise 1. Implement the Sigmoid Function\n",
    "*aka the logistic function*\n",
    "\n",
    "As a very simple warmup, you will implement the sigmoid function. Let's keep this simple!\n",
    "\n",
    "Here's a quick reminder of the formula:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad53d6e01c034eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_function(z):\n",
    "    \"\"\" \n",
    "    Implementation of the sigmoid function by hand\n",
    "    \n",
    "    Args:\n",
    "        z (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        proba (np.float64): the predicted probability for a given observation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtain the predicted probability \n",
    "    # clue: you can use np.exp()\n",
    "    proba = 1/(1+np.exp(-z))\n",
    "    # YOUR CODE HERE\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability: 0.55\n"
     ]
    }
   ],
   "source": [
    "z = 0.2\n",
    "print('Predicted probability: %.2f' % sigmoid_function(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b318c632b90c5f18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probability: 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-218ac735743f5bfb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "z = 1.1\n",
    "assert hashlib.md5(np.round(sigmoid_function(z),2)).hexdigest() == 'c12ba117c0d1805163e06e53c8c3bd81'\n",
    "\n",
    "z = -5\n",
    "assert hashlib.md5(np.round(sigmoid_function(z),2)).hexdigest() == '815d609ae70b810f80cec52c8d9349bc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f40cf4a6bb066df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Make Predictions From Observations\n",
    "\n",
    "The next step is to implement a function that receives observations and returns predicted probabilities.\n",
    "\n",
    "For instance, remember that for an observation with two variables we have:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "where $\\beta_0$ is the intercept and $\\beta_1, \\beta_2$ are the coefficients.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4207cb1317b5cea0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(x_train, coefficients):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns predicted probabilities for a given set of data samples\n",
    "    \n",
    "    Args:\n",
    "        x_train (np.array): a numpy array of shape (m,n)\n",
    "            - m: number of training observations\n",
    "            - n: number of variables\n",
    "        coefficients (np.array): a numpy array of shape (n + 1,)\n",
    "            - coefficients[0]: intercept\n",
    "            - coefficients[1:]: remaining coefficients\n",
    "\n",
    "    Returns:\n",
    "        proba (np.array): the predicted probabilities for a given set of data samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # start by assigning the intercept to z \n",
    "    # clue: the intercept is the first element of the list of coefficients\n",
    "    z = coefficients[0]\n",
    "    # YOUR CODE HERE\n",
    "    z+=np.dot(x_train,coefficients[1:])\n",
    "    \n",
    "    # sum the remaining variable * coefficient products to z\n",
    "    # clue: the variables and coefficients indeces are not exactly aligned, but correctly ordered\n",
    "    #for i in range():                     # replace the empty list with range and iterate through the observation variables (clue: you can use np.shape())\n",
    "    #    z += None                    # multiply the variable value by its coefficient and add to z (clue: use x[:, 0] to access the first variable over all samples)\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    # obtain the predicted probability from z\n",
    "    # clue: we already implemented something that can give us that\n",
    "    proba = sigmoid_function(z)\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb42278953481879",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities:  0.646 and 0.943\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1.3, 1.1, 0.4, 2.2],\n",
    "             [1.0, 2.0, 0.5, 0.1]])\n",
    "coefficients = np.array([1.1,5,-4, 10, -3])\n",
    "\n",
    "print('Predicted probabilities:  %.3f and %.3f' % (predict_proba(x, coefficients)[0], predict_proba(x, coefficients)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2afb353cd17406e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probabilities:  0.646 and 0.943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d746da0f61f9d3a",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[2, 0.1, -1.1], [-1.2, 1.2, 3.2]])\n",
    "coefficients = np.array([-0.1,1,-3, 1.3])\n",
    "assert hashlib.md5(np.round(predict_proba(x, coefficients),2)).hexdigest() == 'e1e3a8f137e127c9c8f047843ba9a187'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6444271eaf86b4f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Compute the Maximum Log-Likelihood Cost Function\n",
    "\n",
    "As you will implement stochastic gradient descent, you only have to do the following for each prediction: \n",
    "\n",
    "$$H_{\\hat{p}}(y) =  - (y \\log(\\hat{p}) + (1-y) \\log (1-\\hat{p}))$$\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-daa84c9da629c861",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def maximum_log_likelihood(y, proba):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the Cross-Entropy loss\n",
    "    \n",
    "    Args:\n",
    "        y (np.int64): an integer\n",
    "        proba (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        loss (np.float): a float with the resulting loss for a given prediction\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the inner right side of the loss function (for when y == 0)\n",
    "    inner_right = (1-y)*np.log(1-proba)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    # compute the inner left side of the loss function (for when y == 1)\n",
    "    # clue: use np.log()\n",
    "    inner_left = y*np.log(proba)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # compute the total loss\n",
    "    total_loss = -(inner_right + inner_left)\n",
    "    # YOUR CODE HERE\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-759edb2d638d14eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed loss:  1.609\n"
     ]
    }
   ],
   "source": [
    "y = 1\n",
    "proba = 0.2\n",
    "print('Computed loss:  %.3f' % maximum_log_likelihood(y, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73aa2b5fc2e95825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Computed loss:  1.609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-328918be0c37238a",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y = 1\n",
    "proba = 0.11\n",
    "assert hashlib.md5(np.round(maximum_log_likelihood(y, proba),3)).hexdigest() == 'c32ab5f1bb15b402788647e939fca19c'\n",
    "\n",
    "y = 1\n",
    "proba = 0.98\n",
    "assert hashlib.md5(np.round(maximum_log_likelihood(y, proba),3)).hexdigest() == '3c0098685eae68ef1f7ce8a5bbd1c641'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb7dca3ebe6d82c8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Obtain the Optimized Coefficients \n",
    "\n",
    "Now that the warmup is done, let's do the most interesting exercise. Here you will implement the optimized coefficients through Batch Gradient Descent.\n",
    "\n",
    "## Quick reminders:\n",
    "\n",
    "For Stochastic gradient descent:\n",
    "\n",
    "$$H_{\\hat{p}}(y) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left [{ y_i \\ \\log(\\hat{p}_i) + (1-y_i) \\ \\log (1-\\hat{p}_i)} \\right ]$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_{0(t)}}$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_t}$$\n",
    "\n",
    "which can be simplified to\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p})\\right]$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p}) \\ x \\right]$$\n",
    "\n",
    "Then for Batch Gradient Descent:\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\frac{1}{N} \\sum_{i=1}^{N} \\left [(y_i - \\hat{p}_i) \\ \\hat{p}_i \\ (1 - \\hat{p}_i) \\ x_i \\right]$$\n",
    "\n",
    "And the intercept:\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\frac{1}{N} \\sum_{i=1}^{N} \\left [(y_i - \\hat{p}_i) \\ \\hat{p}_i \\ (1 - \\hat{p}_i) \\right]$$\n",
    "\n",
    "You will have to initialize a numpy array full of zeros for the coefficients. If you have a training set $X$, you can initialize it this way:\n",
    "```python\n",
    "coefficients = np.zeros(X.shape[1]+1)\n",
    "```\n",
    "\n",
    "where the $+1$ is adding the intercept.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ea9376a785c999d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefficients(x_train, y_train, learning_rate = 0.1, n_epoch = 50, verbose = False):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the optimized intercept and coefficients\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): a numpy array of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of variables\n",
    "        y_train (np.array): a numpy array of shape (m,)\n",
    "        learning_rate (np.float64): a float\n",
    "        n_epoch (np.int64): an integer of the number of full training cycles to perform on the training set\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the coefficients array with zeros\n",
    "    # clue: use np.zeros()\n",
    "    coefficients = np.zeros(x_train.shape[1]+1)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    # run the batch gradient descent algorithm n_epoch times and update the coefficients\n",
    "    for epoch in range(n_epoch):                    # replace the empty list with range and iterate n_epoch times\n",
    "        proba = predict_proba(x_train,coefficients)\n",
    "        # compute the predicted probability\n",
    "        #print('proba: '+ str(proba))\n",
    "        #print('coefs_pre'+ str(coefficients))\n",
    "        coefficients[0] += learning_rate*np.mean((y_train-proba)*proba*(1-proba))\n",
    "        #print((y_train-proba)*proba*(1-proba))\n",
    "        #print('coefs_pos'+ str(coefficients))# update the intercept (clue: use np.mean())      \n",
    "        #print('update_value'+ str((y_train-proba)*proba*(1-proba)))\n",
    "        for i in range(len(coefficients)-1):                    # replace the empty list with range and iterate through the data variables (clue: use np.shape())\n",
    "            #print('yo'+str(np.dot((y_train-proba)*proba*(1-proba),x_train)))\n",
    "            #print(x_train[:,i])\n",
    "            coefficients[i + 1] += learning_rate*np.mean(((y_train-proba)*proba*(1-proba)) * x_train[:,i]) # update each coefficient (clue: use np.mean())\n",
    "        loss =   np.mean(maximum_log_likelihood(y_train, proba))\n",
    "        #print('coefs_pos'+ str(coefficients))\n",
    "        #print(maximum_log_likelihood(y_train,  predict_proba(x_train,coefficients)))# average the obtained cross entropies (clue: use np.mean())\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        if((epoch%10==0) & verbose):\n",
    "            print('>epoch=%d, learning_rate=%.3f, error=%.3f' % (epoch, learning_rate, loss))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89fd2c5d48b7d01b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, learning_rate=0.100, error=0.693\n",
      ">epoch=10, learning_rate=0.100, error=0.607\n",
      ">epoch=20, learning_rate=0.100, error=0.599\n",
      ">epoch=30, learning_rate=0.100, error=0.592\n",
      ">epoch=40, learning_rate=0.100, error=0.584\n",
      ">epoch=50, learning_rate=0.100, error=0.576\n",
      ">epoch=60, learning_rate=0.100, error=0.569\n",
      ">epoch=70, learning_rate=0.100, error=0.562\n",
      ">epoch=80, learning_rate=0.100, error=0.555\n",
      ">epoch=90, learning_rate=0.100, error=0.548\n",
      ">epoch=100, learning_rate=0.100, error=0.541\n",
      ">epoch=110, learning_rate=0.100, error=0.534\n",
      ">epoch=120, learning_rate=0.100, error=0.527\n",
      ">epoch=130, learning_rate=0.100, error=0.521\n",
      ">epoch=140, learning_rate=0.100, error=0.514\n",
      ">epoch=150, learning_rate=0.100, error=0.508\n",
      ">epoch=160, learning_rate=0.100, error=0.502\n",
      ">epoch=170, learning_rate=0.100, error=0.496\n",
      ">epoch=180, learning_rate=0.100, error=0.490\n",
      ">epoch=190, learning_rate=0.100, error=0.484\n",
      "Computed coefficients:\n",
      "[-0.77946284 -0.02035626 -0.04034926  0.23211248]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[1,2,3], [2,5,9], [3,1,4], [8,2,9]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.1\n",
    "n_epoch = 200\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=True)\n",
    "print('Computed coefficients:')\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-47f232d2602cef89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    >epoch=0, learning_rate=0.100, error=0.693\n",
    "    >epoch=10, learning_rate=0.100, error=0.607\n",
    "    >epoch=20, learning_rate=0.100, error=0.599\n",
    "    >epoch=30, learning_rate=0.100, error=0.592\n",
    "    >epoch=40, learning_rate=0.100, error=0.584\n",
    "    >epoch=50, learning_rate=0.100, error=0.576\n",
    "    >epoch=60, learning_rate=0.100, error=0.569\n",
    "    >epoch=70, learning_rate=0.100, error=0.562\n",
    "    >epoch=80, learning_rate=0.100, error=0.555\n",
    "    >epoch=90, learning_rate=0.100, error=0.548\n",
    "    >epoch=100, learning_rate=0.100, error=0.541\n",
    "    >epoch=110, learning_rate=0.100, error=0.534\n",
    "    >epoch=120, learning_rate=0.100, error=0.527\n",
    "    >epoch=130, learning_rate=0.100, error=0.521\n",
    "    >epoch=140, learning_rate=0.100, error=0.514\n",
    "    >epoch=150, learning_rate=0.100, error=0.508\n",
    "    >epoch=160, learning_rate=0.100, error=0.502\n",
    "    >epoch=170, learning_rate=0.100, error=0.496\n",
    "    >epoch=180, learning_rate=0.100, error=0.490\n",
    "    >epoch=190, learning_rate=0.100, error=0.484\n",
    "    Computed coefficients:\n",
    "    [-0.77946284 -0.02035626 -0.04034926  0.23211248]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8ae10de7a8a357b",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[3,1,3], [1,0,9], [3,3,4], [2,-1,10]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.3\n",
    "n_epoch = 200\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=False)\n",
    "assert np.allclose(coefficients, np.array([-0.22368326, -0.98713492, -0.7708793 ,  0.4834609 ]))\n",
    "\n",
    "x_train = np.array([[3,-1,-2], [-6,9,3], [3,-1,4], [5,1,6]])\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=False)\n",
    "assert np.allclose(coefficients, np.array([-0.40768566, -0.21773974,  1.65415715,  0.28374746]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a1387ec5d3ac2d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Normalize Data\n",
    "\n",
    "Just a quick and easy function to normalize the data. It is crucial that your variables are adjusted between $[0;1]$ (normalized) or standardized so that you can correctly analyse some logistic regression coefficients for your possible future employer.\n",
    "\n",
    "You only have to implement this formula\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Don't forget that the `axis` argument is critical when obtaining the maximum, minimum and mean values! As you want to obtain the maximum and minimum values of each individual feature, you have to specify `axis=0`. Thus, if you wanted to obtain the maximum values of each feature of data $X$, you would do the following:\n",
    "\n",
    "```python\n",
    "X_max = np.max(X, axis=0)\n",
    "```\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4e20d91c912030a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    \"\"\" \n",
    "    Implementation of a function that normalizes your data variables\n",
    "    \n",
    "    Args:\n",
    "        X (np.array): a numpy array of shape (m, n)\n",
    "            m: number of observations\n",
    "            n: number of variables\n",
    "\n",
    "    Returns:\n",
    "        normalized_data (np.array): a numpy array of shape (m, n)\n",
    "\n",
    "    \"\"\"\n",
    "    # compute the numerator\n",
    "    # clue: use np.min()\n",
    "    numerator = X-np.min(X,axis=0)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # compute the numerator\n",
    "    # clue: use np.max() and np.min()\n",
    "    denominator = np.max(X,axis=0)-np.min(X,axis=0)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # obtain the normalized data\n",
    "    normalized_X = numerator/denominator\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-334419ee9c78c698",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization:\n",
      "[[ 7  7  3]\n",
      " [ 2  2 11]\n",
      " [ 9  5  2]\n",
      " [ 0  9  5]\n",
      " [10  1  3]\n",
      " [ 1  5  2]]\n",
      "\n",
      "-------------------\n",
      "\n",
      "After normalization:\n",
      "[[0.7        0.75       0.11111111]\n",
      " [0.2        0.125      1.        ]\n",
      " [0.9        0.5        0.        ]\n",
      " [0.         1.         0.33333333]\n",
      " [1.         0.         0.11111111]\n",
      " [0.1        0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[7,7,3], [2,2,11], [9,5,2], [0,9,5], [10,1,3], [1,5,2]])\n",
    "normalized_data = normalize_data(data)\n",
    "print('Before normalization:')\n",
    "print(data)\n",
    "print('\\n-------------------\\n')\n",
    "print('After normalization:')\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-194d7aa04c4e007c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Before normalization:\n",
    "    [[ 7  7  3]\n",
    "     [ 2  2 11]\n",
    "     [ 9  5  2]\n",
    "     [ 0  9  5]\n",
    "     [10  1  3]\n",
    "     [ 1  5  2]]\n",
    "\n",
    "    -------------------\n",
    "\n",
    "After normalization:\n",
    "\n",
    "    [[0.7        0.75       0.11111111]\n",
    "     [0.2        0.125      1.        ]\n",
    "     [0.9        0.5        0.        ]\n",
    "     [0.         1.         0.33333333]\n",
    "     [1.         0.         0.11111111]\n",
    "     [0.1        0.5        0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b95087a05a56d10",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[2,2,11,1], [7,5,1,3], [9,5,2,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert hashlib.md5(normalized_data).hexdigest() == '277af6f0e6721a66ca19931c726aeb86'\n",
    "\n",
    "data = np.array([[1,3,1,3], [9,5,3,1], [2,2,4,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert hashlib.md5(normalized_data).hexdigest() == '2548d399591b9f950ab5fed9cc89a4e5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b63df0cf2303b692",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6: Putting it All Together (Optional)\n",
    "\n",
    "Do this exercise if you want to apply it to a real classification problem.\n",
    "\n",
    "## The Banknote Authentication Dataset\n",
    "\n",
    "There are 1372 items (images of banknotes — think Euro or dollar bill). There are 4 predictor variables (variance of image, skewness, kurtosis, entropy). The variable to predict is encoded as 0 (authentic) or 1 (forgery).\n",
    "\n",
    "Your quest, is to first analyze this dataset from the materials that you've learned in the previous SLUs and then create a logistic regression model that can correctly classify forged banknotes from authentic ones.\n",
    "\n",
    "The data is loaded for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85f8a5b605f1a8fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "columns = ['variance','skewness','kurtosis','entropy', 'forgery']\n",
    "data = pd.read_csv('data/data_banknote_authentication.txt',names=columns).sample(frac=1, random_state=1)\n",
    "X = data.drop(columns='forgery').values\n",
    "y_train = data.forgery.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>forgery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>-3.551000</td>\n",
       "      <td>1.895500</td>\n",
       "      <td>0.186500</td>\n",
       "      <td>-2.440900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1.311400</td>\n",
       "      <td>4.546200</td>\n",
       "      <td>2.293500</td>\n",
       "      <td>0.225410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>-4.017300</td>\n",
       "      <td>-8.312300</td>\n",
       "      <td>12.454700</td>\n",
       "      <td>-1.437500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>-5.119000</td>\n",
       "      <td>6.648600</td>\n",
       "      <td>-0.049987</td>\n",
       "      <td>-6.520600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.628900</td>\n",
       "      <td>0.813220</td>\n",
       "      <td>1.627700</td>\n",
       "      <td>0.776270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.023500</td>\n",
       "      <td>6.901000</td>\n",
       "      <td>-2.006200</td>\n",
       "      <td>-2.712500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>4.646400</td>\n",
       "      <td>10.532600</td>\n",
       "      <td>-4.585200</td>\n",
       "      <td>-4.206000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>3.779100</td>\n",
       "      <td>2.576200</td>\n",
       "      <td>1.309800</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>4.454900</td>\n",
       "      <td>2.497600</td>\n",
       "      <td>1.031300</td>\n",
       "      <td>0.968940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>4.384600</td>\n",
       "      <td>-4.879400</td>\n",
       "      <td>3.366200</td>\n",
       "      <td>-0.029324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-1.180400</td>\n",
       "      <td>11.509300</td>\n",
       "      <td>0.155650</td>\n",
       "      <td>-6.819400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>-5.441400</td>\n",
       "      <td>7.236300</td>\n",
       "      <td>0.109380</td>\n",
       "      <td>-7.564200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1.684900</td>\n",
       "      <td>8.748900</td>\n",
       "      <td>-1.264100</td>\n",
       "      <td>-1.385800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>-1.758900</td>\n",
       "      <td>-6.462400</td>\n",
       "      <td>8.477300</td>\n",
       "      <td>0.319810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>-2.286000</td>\n",
       "      <td>-5.448400</td>\n",
       "      <td>5.803900</td>\n",
       "      <td>0.882310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.803300</td>\n",
       "      <td>9.086200</td>\n",
       "      <td>-3.366800</td>\n",
       "      <td>-1.022400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>5.745600</td>\n",
       "      <td>10.180800</td>\n",
       "      <td>-4.785700</td>\n",
       "      <td>-4.336600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>0.722520</td>\n",
       "      <td>-0.053811</td>\n",
       "      <td>5.670300</td>\n",
       "      <td>-1.350900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>4.142500</td>\n",
       "      <td>-3.679200</td>\n",
       "      <td>3.828100</td>\n",
       "      <td>1.629700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-1.327400</td>\n",
       "      <td>9.498000</td>\n",
       "      <td>2.440800</td>\n",
       "      <td>-5.268900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>-0.690780</td>\n",
       "      <td>-0.500770</td>\n",
       "      <td>-0.354170</td>\n",
       "      <td>0.474980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>3.106000</td>\n",
       "      <td>9.541400</td>\n",
       "      <td>-4.253600</td>\n",
       "      <td>-4.003000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3.910200</td>\n",
       "      <td>6.065000</td>\n",
       "      <td>-2.453400</td>\n",
       "      <td>-0.682340</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>-0.408570</td>\n",
       "      <td>3.097700</td>\n",
       "      <td>-2.960700</td>\n",
       "      <td>-2.689200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>1.581000</td>\n",
       "      <td>0.869090</td>\n",
       "      <td>-2.313800</td>\n",
       "      <td>0.824120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-3.358200</td>\n",
       "      <td>-7.240400</td>\n",
       "      <td>11.441900</td>\n",
       "      <td>-0.571130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>3.966000</td>\n",
       "      <td>3.921300</td>\n",
       "      <td>0.705740</td>\n",
       "      <td>0.336620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>3.258500</td>\n",
       "      <td>-4.461400</td>\n",
       "      <td>3.802400</td>\n",
       "      <td>-0.150870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>-1.838700</td>\n",
       "      <td>-6.301000</td>\n",
       "      <td>5.650600</td>\n",
       "      <td>0.195670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.456600</td>\n",
       "      <td>9.522800</td>\n",
       "      <td>-4.011200</td>\n",
       "      <td>-3.594400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>-1.855400</td>\n",
       "      <td>-9.603500</td>\n",
       "      <td>7.776400</td>\n",
       "      <td>-0.977160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.731700</td>\n",
       "      <td>-0.347650</td>\n",
       "      <td>4.190500</td>\n",
       "      <td>-0.991380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>-3.895200</td>\n",
       "      <td>3.815700</td>\n",
       "      <td>-0.313040</td>\n",
       "      <td>-3.819400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>3.154100</td>\n",
       "      <td>-5.171100</td>\n",
       "      <td>6.599100</td>\n",
       "      <td>0.574550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>-0.036127</td>\n",
       "      <td>1.525000</td>\n",
       "      <td>-1.408900</td>\n",
       "      <td>-0.761210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>-2.445800</td>\n",
       "      <td>1.628500</td>\n",
       "      <td>-0.885410</td>\n",
       "      <td>-1.480200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>4.707200</td>\n",
       "      <td>8.295700</td>\n",
       "      <td>-2.560500</td>\n",
       "      <td>-1.490500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>-2.460400</td>\n",
       "      <td>12.730200</td>\n",
       "      <td>0.917380</td>\n",
       "      <td>-7.641800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>4.393700</td>\n",
       "      <td>0.357980</td>\n",
       "      <td>2.041600</td>\n",
       "      <td>1.200400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>0.570600</td>\n",
       "      <td>-0.024841</td>\n",
       "      <td>1.242100</td>\n",
       "      <td>-0.562080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.573400</td>\n",
       "      <td>9.193800</td>\n",
       "      <td>-0.909400</td>\n",
       "      <td>-1.872000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>-1.390700</td>\n",
       "      <td>-1.378100</td>\n",
       "      <td>2.305500</td>\n",
       "      <td>-0.021566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>4.570700</td>\n",
       "      <td>7.209400</td>\n",
       "      <td>-3.279400</td>\n",
       "      <td>-1.494400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>-2.534600</td>\n",
       "      <td>-0.773920</td>\n",
       "      <td>3.360200</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>3.205100</td>\n",
       "      <td>8.688900</td>\n",
       "      <td>-2.903300</td>\n",
       "      <td>-0.781900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0.539360</td>\n",
       "      <td>3.894400</td>\n",
       "      <td>-4.816600</td>\n",
       "      <td>-4.341800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-2.327700</td>\n",
       "      <td>1.438100</td>\n",
       "      <td>-0.821140</td>\n",
       "      <td>-1.286200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>-0.703460</td>\n",
       "      <td>2.957000</td>\n",
       "      <td>-3.594700</td>\n",
       "      <td>-3.145700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>-3.895300</td>\n",
       "      <td>4.039200</td>\n",
       "      <td>-0.301900</td>\n",
       "      <td>-2.183600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>5.061700</td>\n",
       "      <td>-0.357990</td>\n",
       "      <td>0.446980</td>\n",
       "      <td>0.998680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.882980</td>\n",
       "      <td>0.660090</td>\n",
       "      <td>6.009600</td>\n",
       "      <td>-0.432770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>3.466300</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.338800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3.848100</td>\n",
       "      <td>10.153900</td>\n",
       "      <td>-3.856100</td>\n",
       "      <td>-4.222800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>-2.250100</td>\n",
       "      <td>3.312900</td>\n",
       "      <td>-0.883690</td>\n",
       "      <td>-2.897400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>-2.482400</td>\n",
       "      <td>-7.304600</td>\n",
       "      <td>6.839000</td>\n",
       "      <td>-0.590530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>3.491600</td>\n",
       "      <td>8.570900</td>\n",
       "      <td>-3.032600</td>\n",
       "      <td>-0.591820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.745210</td>\n",
       "      <td>3.635700</td>\n",
       "      <td>-4.404400</td>\n",
       "      <td>-4.141400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>-4.366700</td>\n",
       "      <td>6.069200</td>\n",
       "      <td>0.572080</td>\n",
       "      <td>-5.466800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2.046600</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>2.176100</td>\n",
       "      <td>-0.083634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>-2.314700</td>\n",
       "      <td>3.666800</td>\n",
       "      <td>-0.696900</td>\n",
       "      <td>-1.247400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance   skewness   kurtosis   entropy  forgery\n",
       "1240 -3.551000   1.895500   0.186500 -2.440900        1\n",
       "703   1.311400   4.546200   2.293500  0.225410        0\n",
       "821  -4.017300  -8.312300  12.454700 -1.437500        1\n",
       "1081 -5.119000   6.648600  -0.049987 -6.520600        1\n",
       "37    3.628900   0.813220   1.627700  0.776270        0\n",
       "167   1.023500   6.901000  -2.006200 -2.712500        0\n",
       "223   4.646400  10.532600  -4.585200 -4.206000        0\n",
       "647   3.779100   2.576200   1.309800  0.565500        0\n",
       "325   4.454900   2.497600   1.031300  0.968940        0\n",
       "558   4.384600  -4.879400   3.366200 -0.029324        0\n",
       "341  -1.180400  11.509300   0.155650 -6.819400        0\n",
       "1218 -5.441400   7.236300   0.109380 -7.564200        1\n",
       "302   1.684900   8.748900  -1.264100 -1.385800        0\n",
       "1124 -1.758900  -6.462400   8.477300  0.319810        1\n",
       "793  -2.286000  -5.448400   5.803900  0.882310        1\n",
       "80    2.803300   9.086200  -3.366800 -1.022400        0\n",
       "607   5.745600  10.180800  -4.785700 -4.336600        0\n",
       "538   0.722520  -0.053811   5.670300 -1.350900        0\n",
       "255   4.142500  -3.679200   3.828100  1.629700        0\n",
       "236  -1.327400   9.498000   2.440800 -5.268900        0\n",
       "1209 -0.690780  -0.500770  -0.354170  0.474980        1\n",
       "108   3.106000   9.541400  -4.253600 -4.003000        0\n",
       "48    3.910200   6.065000  -2.453400 -0.682340        0\n",
       "1109 -0.408570   3.097700  -2.960700 -2.689200        1\n",
       "1338  1.581000   0.869090  -2.313800  0.824120        1\n",
       "1126 -3.358200  -7.240400  11.441900 -0.571130        1\n",
       "336   3.966000   3.921300   0.705740  0.336620        0\n",
       "186   3.258500  -4.461400   3.802400 -0.150870        0\n",
       "1259 -1.838700  -6.301000   5.650600  0.195670        1\n",
       "3     3.456600   9.522800  -4.011200 -3.594400        0\n",
       "...        ...        ...        ...       ...      ...\n",
       "1031 -1.855400  -9.603500   7.776400 -0.977160        1\n",
       "141   1.731700  -0.347650   4.190500 -0.991380        0\n",
       "1110 -3.895200   3.815700  -0.313040 -3.819400        1\n",
       "753   3.154100  -5.171100   6.599100  0.574550        0\n",
       "1001 -0.036127   1.525000  -1.408900 -0.761210        1\n",
       "1239 -2.445800   1.628500  -0.885410 -1.480200        1\n",
       "580   4.707200   8.295700  -2.560500 -1.490500        0\n",
       "562  -2.460400  12.730200   0.917380 -7.641800        0\n",
       "398   4.393700   0.357980   2.041600  1.200400        0\n",
       "668   0.570600  -0.024841   1.242100 -0.562080        0\n",
       "252   0.573400   9.193800  -0.909400 -1.872000        0\n",
       "907  -1.390700  -1.378100   2.305500 -0.021566        1\n",
       "468   4.570700   7.209400  -3.279400 -1.494400        0\n",
       "914  -2.534600  -0.773920   3.360200  0.001710        1\n",
       "357   3.205100   8.688900  -2.903300 -0.781900        0\n",
       "1278  0.539360   3.894400  -4.816600 -4.341800        1\n",
       "1300 -2.327700   1.438100  -0.821140 -1.286200        1\n",
       "1202 -0.703460   2.957000  -3.594700 -3.145700        1\n",
       "1305 -3.895300   4.039200  -0.301900 -2.183600        1\n",
       "508   5.061700  -0.357990   0.446980  0.998680        0\n",
       "749   0.882980   0.660090   6.009600 -0.432770        0\n",
       "129   3.466300   1.111200   1.742500  1.338800        0\n",
       "144   3.848100  10.153900  -3.856100 -4.222800        0\n",
       "960  -2.250100   3.312900  -0.883690 -2.897400        1\n",
       "847  -2.482400  -7.304600   6.839000 -0.590530        1\n",
       "715   3.491600   8.570900  -3.032600 -0.591820        0\n",
       "905   0.745210   3.635700  -4.404400 -4.141400        1\n",
       "1096 -4.366700   6.069200   0.572080 -5.466800        1\n",
       "235   2.046600   2.030000   2.176100 -0.083634        0\n",
       "1061 -2.314700   3.666800  -0.696900 -1.247400        1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d788f651986616fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will also have to return several values, such as the number of forged and authentic banknotes. To do so, remember that you can do masks in numpy arrays. If you had a numpy array of labels called `labels` and wanted to obtain the ones with label $1$, you would do the following:\n",
    "\n",
    "```python\n",
    "filtered_labels = labels[labels==1]\n",
    "```\n",
    "\n",
    "You will additionally be asked to obtain the number of correct forged banknotes predictions. Imagine that you have a numpy array with the predictions called `predictions` and a numpy array with the correct labels called `labels` and you wanted to obtain the number of correct predictions of a label $1$. You would do the following:\n",
    "\n",
    "```python\n",
    "n_correct_predictions = labels[(labels==1) & (predictions==1)].shape[0]\n",
    "```\n",
    "\n",
    "Also, don't forget to use these values for your logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e580d4b8a0715014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.3\n",
    "n_epoch = 200\n",
    "\n",
    "# For validation\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc2bba765f6718cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now let's do this!\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12fca244a20276be",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of forged banknotes: 610\n",
      "\n",
      "The last three normalized rows:\n",
      "[[0.19293425 0.74247045 0.25236091 0.28018586]\n",
      " [0.65542407 0.59132937 0.3214595  0.76966693]\n",
      " [0.34091253 0.65257608 0.19769531 0.6638479 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# STEP ONE: Initial analysis and data processing\n",
    "# How many banknotes are forged? (clue: use y_train)\n",
    "n_forged = y_train.sum()\n",
    "# YOUR CODE HERE\n",
    "# How many banknotes are authentic? (clue: use y_train)\n",
    "n_authentic = len(y_train) - n_forged\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Normalize the training data X (clue: we have already implemented this)\n",
    "x_train = normalize_data(X)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Number of forged banknotes: %i\" % n_forged)\n",
    "\n",
    "print(\"\\nThe last three normalized rows:\")\n",
    "print(x_train[-3:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d36da31b08c8c036",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "Expected output:\n",
    "\n",
    "    Number of forged banknotes: 610\n",
    "\n",
    "    The last three normalized rows:\n",
    "    [[0.19293425 0.74247045 0.25236091 0.28018586]\n",
    "     [0.65542407 0.59132937 0.3214595  0.76966693]\n",
    "     [0.34091253 0.65257608 0.19769531 0.6638479 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38cbff1a547b1418",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The last three coefficients:\n",
      "[-0.08321107  0.00576783 -0.02943804]\n",
      "\n",
      "The last three obtained probas:\n",
      "[0.46747167 0.45417692 0.46229941]\n",
      "\n",
      "The last three predictions:\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# STEP TWO: Model training and predictions\n",
    "# What coefficients can we get? (clue: we have already implemented this)\n",
    "# note: don't forget to use all the hyperparameters defined above\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate = 0.1, n_epoch = 50, verbose = False)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# What are the predicted probabilities on the training data?\n",
    "probas = predict_proba(x_train,coefficients)   # get the predicted probabilities (clue: we already implemented this)\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "\n",
    "# If we had to say whether a banknote was forged or not, what are the predictions?\n",
    "# clue 1: Hard assign the predicted probabilities by rounding them to the nearest integer\n",
    "# clue 2: use np.round()\n",
    "preds = np.round(probas)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"\\nThe last three coefficients:\")\n",
    "print(coefficients[-3:])\n",
    "\n",
    "print(\"\\nThe last three obtained probas:\")\n",
    "print(probas[-3:])\n",
    "\n",
    "print(\"\\nThe last three predictions:\")\n",
    "print(preds[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b84678df577950a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    >epoch=0, learning_rate=0.300, error=0.693\n",
    "    >epoch=10, learning_rate=0.300, error=0.680\n",
    "    >epoch=20, learning_rate=0.300, error=0.673\n",
    "    >epoch=30, learning_rate=0.300, error=0.667\n",
    "    >epoch=40, learning_rate=0.300, error=0.662\n",
    "    >epoch=50, learning_rate=0.300, error=0.658\n",
    "    >epoch=60, learning_rate=0.300, error=0.654\n",
    "    >epoch=70, learning_rate=0.300, error=0.650\n",
    "    >epoch=80, learning_rate=0.300, error=0.647\n",
    "    >epoch=90, learning_rate=0.300, error=0.643\n",
    "    >epoch=100, learning_rate=0.300, error=0.639\n",
    "    >epoch=110, learning_rate=0.300, error=0.636\n",
    "    >epoch=120, learning_rate=0.300, error=0.632\n",
    "    >epoch=130, learning_rate=0.300, error=0.629\n",
    "    >epoch=140, learning_rate=0.300, error=0.626\n",
    "    >epoch=150, learning_rate=0.300, error=0.623\n",
    "    >epoch=160, learning_rate=0.300, error=0.620\n",
    "    >epoch=170, learning_rate=0.300, error=0.617\n",
    "    >epoch=180, learning_rate=0.300, error=0.614\n",
    "    >epoch=190, learning_rate=0.300, error=0.611\n",
    "\n",
    "    The last three coefficients:\n",
    "    [-0.46533509  0.21801537  0.13774932]\n",
    "\n",
    "    The last three obtained probas:\n",
    "    [0.46480018 0.40605379 0.45419773]\n",
    "\n",
    "    The last three predictions:\n",
    "    [0. 0. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[preds==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8387dc18c3e0f789",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct forged predictions: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP THREE: Results analysis\n",
    "# How many banknotes were predicted as forged? (clue: use preds and len() or .shape)\n",
    "n_predicted_forged = (preds==1).sum()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# How many forged banknotes were correctly detected? (clue: use y_train, preds and len() or .shape)\n",
    "n_correct_forged_predictions = ((y_train==1) & (preds==1)).sum()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Number of correct forged predictions: %i\" % n_correct_forged_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f04909dc6afe1e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Number of correct forged predictions: 165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84ca2370adc7d541",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a dataset with 610 forged banknotes and 762 authentic banknotes. \n",
      "\n",
      "After analysing the data and training your own logistic regression classifier you find out that it correctly identified 0 out of 0 forged banknotes. But you know that it still did not detect most of the forged banknotes.\n"
     ]
    }
   ],
   "source": [
    "print('You have a dataset with %s forged banknotes and %s authentic banknotes. \\n\\n'\n",
    "     'After analysing the data and training your own logistic regression classifier you find out that it correctly '\n",
    "     'identified %s out of %s forged banknotes. But you know that it still did not detect most of the forged banknotes.'% \n",
    "      (n_forged, n_authentic, n_predicted_forged, n_correct_forged_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from scipy.sparse import random, coo_matrix, lil_matrix, dok_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU10 - Learning Notebook - Part 1 of 3 - Introduction to Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Introduction\n",
    "\n",
    "## 0.1 What are Recommender Systems? Are they really important?\n",
    "\n",
    ">\"35 percent of what consumers purchase on Amazon and 75 percent of what they watch on Netflix come from product recommendations based on such algorithms.\" [(link)](https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers)\n",
    "\n",
    "\n",
    "**Recommender Systems** (RS) are widely used in companies providing a wide range of similar content (e.g. movies/shows on Netflix, songs/podcasts on Spotify, items on Amazon). Since these companies cannot ask all users to rate every single content (Spotify has [+50 million tracks and 232 million active users)](https://newsroom.spotify.com/company-info/), recommender systems enable the companies to suggest new content for both users which already shown their preferences and to new customers they don't know anything about.\n",
    "\n",
    "---\n",
    "\n",
    "*That seems nice, but why can't we use Classification or Regression Models we already know?*\n",
    "\n",
    "The recommender system aims to predict the best ranking possible of items for an user. If we are trying to predict a rating, and a higher raking is a better score, why can't we consider this as a classical regression or an ordinal classification \\*[1] problem? The main aspect of recommender systems (when compared to typical classification/regression problems) is that we are solving a high-sparsity problem on our inputs. We normally have a severe imbalance for unlabeled data, so we consider this task as a matrix completion problem.\n",
    "\n",
    "\n",
    ">*[1] Ordinal classification considers the label as a classification problem where the order has meaning (e.g. predicting the rating of a movie where an higher rating indicates a better movie). Typical classifications problems, such as predicting the weather labels (rain vs sunny vs cloudy) do not have an intrinsic order (e.g. \"sunny is lesser than cloudy\").*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Notorious Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Youtube**: Google-powered, their video recommendation system use content (e.g. metadata) and user activity (implicit and explicit) data and it is responsible for 60% of video clicks from the home page. For curious minds, here is a link for their [(paper)](https://www.researchgate.net/profile/Sujoy_Gupta2/publication/221140967_The_YouTube_video_recommendation_system/links/53e834410cf21cc29fdc35d2/The-YouTube-video-recommendation-system.pdf)\n",
    "\n",
    "**Spotify**: the Swedish company uses a mix of Collaborative Filtering, NLP, Raw Audio models and Music Curators to suggest content for its users. Their recommender system is feeded not by explicit ratings but by users' interaction with the software (implicit feedback). \n",
    "\n",
    "**Netflix**: by mixing implicit and explicit feedback, the media-services provider uses an interface with top-down right-to-left ranking for suggested personalized content. They use your interactions with the service, similarity to other members' tastes and metadata about the titles (e.g. genre, actors) to feed their Recommendation System. Simple English explanation from Netflix themselves [(here)](https://help.netflix.com/en/node/100639)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Recommender systems\n",
    "- <span style=\"color:darkred\">Non-personalized systems</span>: Recommendations are the same for every user\n",
    "    - **Best-Seller**\n",
    "    - **Trending Hot**\n",
    "    - **Highest rated**\n",
    "    - **People who like X also like Y**\n",
    "    \n",
    "    \n",
    "- <span style=\"color:darkred\">Personalized systems</span>: Recommendations are the specific for each user\n",
    "\n",
    "    - **Collaborative Filtering**: Based only on the users past behavior \n",
    "        - **User-based**: Find similar users to me and recommend what they liked\n",
    "        - **Item-based**: Find similar items to those that I have previously liked\n",
    "    - **Content-based**: Recommend based on item features \n",
    "    - **Demographic**: Recommend based on user features\n",
    "    - **Personalized Learning to Rank**: Treat recommendation as a ranking problem \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 0.3 Agenda\n",
    "The main goal of BLU10 is to teach how to build \"Non-personalized recommender systems\". After that, BLU11 will teach how to build \"Personalized recommender systems\" and then we will end with the \"Workflow\" in BLU12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Framework\n",
    "\n",
    "**Recommendation Systems (RS)** are software systems that recommend items to users, that they might like.\n",
    "\n",
    "We start by learning the main components of an RS.\n",
    "\n",
    "![Recommender Sytems Framework](./media/recommender_systems_framework.png)\n",
    "\n",
    "*Fig.1 - RS framework with a community, plus the basic and extended models.*\n",
    "\n",
    "We refer back to this framework frequently throughout the specialization, but for now, let's drill down into each component.\n",
    "\n",
    "## 1.1 Users\n",
    "\n",
    "The *consumers* or people, denoted by $U = \\{u_0, u_1, ..., u_m\\}$, where the number of users $\\left\\vert{U}\\right\\vert = m$.\n",
    "\n",
    "We reserve the indexing letters $u$ and $v$ to denote generic individual users.\n",
    "\n",
    "## 1.2 Items\n",
    "\n",
    "The *products* or things, a set $I = \\{i_0, i_1, ..., i_n\\}$, with the number of elements $\\left\\vert{I}\\right\\vert = n$. \n",
    "\n",
    "The indexing letters for letters for items are $i$, $j$ and $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Ratings\n",
    "\n",
    "**Ratings** are the *transactions* or *opinions*, provided by the users about the items.\n",
    "\n",
    "We write the set $R = \\{r_{u_0, i_0}, ..., r_{u_m, i_n}\\}$, where each rating $r_{u, i}$ corresponds to a user-item pair $(u, i) \\in U \\times I$.\n",
    "\n",
    "* Any user $u \\in U$ can make no more than one rating $r_{u, i}$ for a particular item $i \\in I$.\n",
    "* Any user $u \\in U$ is free to rate, or not, any number of items $i \\in I$, including none.\n",
    "\n",
    "Non-personalized recommender systems usually make exclusive use of this matrix in orde to make predictions. These will be the first ones we'll study.\n",
    "\n",
    "\n",
    "\n",
    "## 1.5 User model\n",
    "\n",
    "As introduced above, RS are in the business of matching users and items.\n",
    "\n",
    "Sometimes, it's convenient to have user and item profiles in the same attribute space $A$.\n",
    "\n",
    "The user model $M$, defines $M = \\{m_{u_0, a_0}, ..., m_{u_m, a_r}\\}$, for $(u, a) \\in U \\times A$, where $A$ is the set of item attributes.\n",
    "\n",
    "This matrix is normally used for collaborative filtering systems (or user-based filtering). These are the first of the the personalized systems (BLU11) we will learn.\n",
    "\n",
    "\n",
    "## 1.6 Profiles\n",
    "\n",
    "**Profiles** are a collection of objects (users or items, in our framework) and their **attributes**.\n",
    "\n",
    "Consider the set of attributes $A = \\{a_0, ..., a_r\\}$, where $r \\in \\mathbb{N_0}$. Let be $A$ an arbitrary set of **item attributes**.\n",
    "\n",
    "We can define profiles $P = \\{p_{i_0, a_0}, ..., p_{i_n, a_r}\\}$, where $(i, a) \\in I \\times A$ and values $p_{i, a}$ indicate the presence of $a$ in $i$.\n",
    "\n",
    "Let be $B$ be an arbitrary set of user characteristics, and we can apply the same reasoning to build **user profiles**.\n",
    "\n",
    "Typically, item profiles contain information about the content of the items, and user profiles are more focused on demographics.\n",
    "\n",
    "This matrix is normally used for content-based filtering systems, the last ones we'll learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Ratings matrix\n",
    "\n",
    "The community matrix hints at the canonical representation of the ratings matrix, at the core of any RS.\n",
    "\n",
    "We represent the set $R = \\{r_{u_0, i_0}, ..., r_{u_m, i_n}\\}$ as a $U \\times I$  matrix - the **ratings matrix** -, where the values are the ratings $r_{u, i}$, if they exist:\n",
    "\n",
    "\n",
    "<img align=\"center\" width=\"413\" height=\"239\" src=\"./media/ratings_matrix3.png\">\n",
    "\n",
    "\n",
    "We represent not recorded ratings as zeros or missing values, enforcing the $U \\times I$ shape.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Non-Personalized Recommendations\n",
    "\n",
    "The whole objective of Recommender Systems is to fill in the blanks in our ratings matrix and return the best possible items to the user.\n",
    "\n",
    "Throughout the course, we learn different ways to predict unseen ratings, but the outputs remain unchanged.\n",
    "## (TODO: Write the following from scratch, to make these clearer)\n",
    "## 4.1 Prediction step\n",
    "\n",
    "The RS core computation is to predict the utility of unseen items $i \\in I \\setminus I_u$ to a user $u$, where $I \\setminus I_u$  is the subset of items rated by user $u.\n",
    "\n",
    "At the core, we learn a function $f$ that maps user-item pairs into ratings $f : U \\times I \\to S$ given by $\\hat{r}_{u, i} = f(u, i)$ where $S$ is the set of possible ratings.\n",
    "\n",
    "Once we have it, there two main types of recommendations: top-$N$ and best-item.\n",
    "\n",
    "## 4.2 Top-*K* items\n",
    "\n",
    "For a given user $u \\in U$, we need a set of predictions $\\hat{R}_u = \\{f(u, i) : (u, i) \\in u \\times (I \\setminus I_u)\\}$.\n",
    "\n",
    "Then, we take a subset $L_u \\subset (I \\setminus I_u$), containing the items with the *k*-largest predicted ratings $\\hat{r}_{u, i} \\in \\hat{R}_u$.\n",
    "\n",
    "Optionally, $L_u = \\{i_0, i_1, ..., i_k\\}$ can be ordered as $\\hat{r}_{u, i_0} \\geq \\hat{r}_{u, i_1} \\geq ... \\geq \\hat{r}_{u, i_k}$.\n",
    "\n",
    "## 4.3 Best-item\n",
    "\n",
    "Can be seen as a particular case of top-$K$, with $K = 1$.\n",
    "\n",
    "Thus, the function $f(u, i)$ can be used to find the best item, such as $j = \\underset{i \\space \\in \\space I \\setminus I_u}{\\mathrm{argmax}} \\space f(u, i)$.\n",
    "\n",
    "\n",
    "\n",
    "As you see, there are many operations that can be applied to the ratings matrix in order to extract useful information for all the users, so...\n",
    "\n",
    "<img align=\"center\" width=\"413\" height=\"239\" src=\"media/meme_rmatrix.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "# 5 Context-awareness\n",
    "\n",
    "Finally, some systems consider the context, alongside users and items. Take $C$ as a set of contexts.\n",
    "\n",
    "The reasoning is that sometimes the utility for an item is observed to depend on other variables.\n",
    "\n",
    "(A very good camera, for example, may be of lesser utility for a newbie than it is for pro, for example.)\n",
    "\n",
    "In these cases, we need $f$ to use the context as well, as $f : U \\times I \\times C \\to S$ given by $\\hat{r}_{u, i, c} = f(u, i, c)$.\n",
    "\n",
    "For a given user $u \\in U$, the predictions become $\\hat{R}_u = \\{f(u, i, c) : (u, i, c) \\in u \\times (I \\setminus I_u) \\times C\\}$.\n",
    "\n",
    "## 5.1 Time\n",
    "\n",
    "We can think of time as a particular case of context-aware RS, where $\\hat{r}_{u, i, t} = f(u, i, t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sparse Matrices\n",
    "\n",
    "Huge matrices require much memory, and some large matrices are very sparse, as recorded ratings are relatively rare. Thing of Netflix: you, as a user, have not provided ratings for the vast majority (if any) of the movies and TV shows. This means that most of the recorded ratings matrix is full of zeros or missing values and only a few entries are filled with information.\n",
    "\n",
    "This allocation is a waste of resources, as missing values and data cost the same space, but only the later hold any information.\n",
    "\n",
    "In practice, this leads to matrices that don't fit in memory, despite having a manageable amount of data.\n",
    "\n",
    "The premise of sparse data structures is that we *store only non-zero values*, and assume the rest of them are zeros.\n",
    "\n",
    "**Sparse matrices** allow us to mitigate these problems:\n",
    "* They are less memory-intensive, as they squeeze out the zeros and store only relevant values;\n",
    "* Operations ignore zero values, i.e., the majority of the cells.\n",
    "\n",
    "## 2.1 Sparse Matrices in SciPy\n",
    "\n",
    "The `scipy.sparse` module implements sparse matrices based in regular NumPy arrays.\n",
    "\n",
    "For the sake of objectivity, let's compare the sizes of a sparse versus a regular matrix.\n",
    "\n",
    "We use `sp.sparse.random` to generate a sparse matrix of a given shape and density, with randomly distributed values.\n",
    "\n",
    "(The term density is often used to refer to what we called the sparsity score previously.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015005005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_matrix_nbytes(M):\n",
    "    return M.data.nbytes + M.indptr.nbytes + M.indices.nbytes\n",
    "\n",
    "\n",
    "A = random(10 ** 3, 10 ** 5, density=.01, format='csr')\n",
    "sparse_matrix_nbytes(A) / A.toarray().nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Dictionary of Keys (DOK)\n",
    "\n",
    "The most straightforward implementation of a sparse matrix is as a dictionary of keys, in which the keys are tuples represent indices.\n",
    "\n",
    "```\n",
    "┌───┬───┬───┐          \n",
    "│ 2 │ 0 │ 0 │          {  \n",
    "├───┼───┼───┤            (0, 0): 2,\n",
    "│ 0 │ 5 │ 0 │ → DoK →    (1, 1): 5,\n",
    "├───┼───┼───┤            (2, 1): 3,\n",
    "│ 0 │ 3 │ 0 │          }\n",
    "└───┴───┴───┘ \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that, if our matrix was 100x100 instead of 3x3, and it had the same only 3 nonzero elements:\n",
    "```\n",
    "┌───┬───┬───┬───┬───┬         \n",
    "│ 2 │ 0 │ 0 │ 0 │...│        \n",
    "├───┼───┼───┼───┼───┼ \n",
    "│ 0 │ 5 │ 0 │ 0 │...│ \n",
    "├───┼───┼───┼───┼───┼\n",
    "│ 0 │ 3 │ 0 │ 0 │...│       \n",
    "├───┼───┼───┼───┼───┼\n",
    "│ 0 │ 0 │ 0 │ 0 │...│ \n",
    "├───┼───┼───┼───┼───┼\n",
    "│...│...│...│...│...│\n",
    "```\n",
    "The DOK will be the same:\n",
    "```\n",
    "{  \n",
    "    (0, 0): 2,\n",
    "    (1, 1): 5,\n",
    "    (2, 1): 3\n",
    "}\n",
    "```\n",
    "\n",
    "We could save a lot of space by only storing that dictionary instead of every single zero element int matrix (10000 elements!)\n",
    "\n",
    "There are methods that are even better at optimizing the amount of memory saved. We will use, by default, the CSR method for storing sparse matrices. You can check its details and compare with other methods in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2],\n",
       "       [1, 5, 0],\n",
       "       [0, 2, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1, 0, 2, 1, 5, 0, 0, 2, 1]).reshape(3, 3)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_ = csr_matrix(data)\n",
    "H_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
